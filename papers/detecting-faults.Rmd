---
title: "Detection of anomalies in environmental sensor data via sequence analysis"
author: "JJ Merelo +"
date: "19 de marzo de 2017"
output:
  pdf_document:
    keep_tex: true
bibliography: refs.bib
abstract:  "After a hackathon where we studied data gathered from the
environmental sensors in the city of Granada and other cities in
Andalucia, we found several oddities in the data sequences, which
makes us doubt the veracity of published data. In this paper we
present several methods to study those oddities or possible faults
that could eventually be used to validate data provided by smart
city sensors or other kind of data providers." 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The whole point of the open data movement is to keep the
administration in check so that the citizens can take informed
decisions about their continuity, but it is also about reusing data
provided by the administration for their own economic, health or
political benefit.

The first step of opening data is obtaining data provided by the
administration via freeing it from proprietary formats or simply
documents that prevent quantitative or qualitative analysis of the
data. In this first step, we should believe that the only source of
errors or *noise* is the extraction process and that data itself is
clean and should be trusted, since it arrives from qualified sources
or calibrated sensors.

During the beginning of the month of March 2017, we initiated a
process that extracted air quality information from pages hosted by
the regional government, the Junta de Andaluc√≠a. After some
visualization, we realized long sequences of measurements that yielded
exactly the same number and other changes that did not seem have
correlation with reality.

This paper in an attempt to present a methodology that would find
problems in time series of environmental measurements via a series of
statistical analysis. After a brief presentation of the state of the
art, we show the results of the initial data exploration we performed,
followed by the different techniques used to analyze the possibility
that the data was faulty. Finally, we will present our conclusions.

# State of the art

If we focus on detection of anomalies in open data, the field is
relatively new and this area has not been explored extensively. In
general, detecting data points that do not adjust to a *normal*
pattern is called anomaly or outlier detection. Several survey papers
[@chandola2009anomaly,@chandola2012anomaly] review the state of the
and different techniques that can be applied. In our case, data is not
abnormal by itself, except when missing; it can be considered abnormal
within the sequence of data: either big changes or no changes are not
consistent with the nature of what is measured, particle or gas
concentrations. Since the data is anomalous in the context of the time
series, this is normally called *contextual* anomaly detection, since
the consideration of normal or not makes sense only within the
context.

Even in this particular context, most papers are devoted to finding anomalies in
financial data [@fanning1998neural,@5522816]. However, some relatively
recent papers have focused on detection of anomalies in data taken
from environmental sensors [@hill2007real,@hill2010anomaly]. They
mention the presence of sensor and transmission errors, but more
frequently anomalies are actual deviations from historical data such
as heavy weather events or that caused by environmental
disasters. They use dynamic Bayesian networks to find anomalies in
windspeed data, finding a high rate of false negatives but concluding
that the speed of detection of these anomalies is adequate and,
besides, can use data as it becomes available, being able to scale
also to large quantities of data. A more recent paper
[@hill2010anomaly] uses auto-regression to compare data predicted at
`t+1` with actual data. Besides, this paper states that it is not
realistic to accurately label all possible anomalous instances, since
anomalous behavior can arise from different causes; that is why high
levels of accuracy in classification should not be expected.

In our case, we will use a non-supervised method and we will try to
establish thresholds that will tell us when some data deviates from
*normality*. 


# Initial data exploration

# Finding faults in environmental time series

## Using entropy

## Using time-wise deltas


## References
